{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss9WR_zbQELO"
      },
      "source": [
        "# Практическое задание 3\n",
        "\n",
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB3vM2GfQELQ"
      },
      "source": [
        "## Введение\n",
        "\n",
        "### Постановка задачи\n",
        "\n",
        "В этом задании вы будете решать задачу извлечения именованных сущностей (Named Entity Recognition) - одну из самых распространенных в NLP наряду с задачей текстовой классификации.\n",
        "\n",
        "Данная задача заключается в том, что нужно классифицировать каждое слово / токен на предмет того, является ли оно частью именованной сущности (сущность может состоять из нескольких слов / токенов) или нет.\n",
        "\n",
        "Например, мы хотим извлечь имена и названия организаций. Тогда для текста\n",
        "\n",
        "    Yan    Goodfellow  works  for  Google  Brain\n",
        "\n",
        "модель должна извлечь следующую последовательность:\n",
        "\n",
        "    B-PER  I-PER       O      O    B-ORG   I-ORG\n",
        "\n",
        "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система (*BIO*-разметка) введена, чтобы различать последовательные именованные сущности одного типа.\n",
        "Существуют и другие типы разметок, например *BILUO*, но в рамках данного практического задания сфокусируемся имеено на *BIO*.\n",
        "\n",
        "Решать NER задачу мы будем на датасете CoNLL-2003 с использованием рекуррентных сетей и моделей на базе архитектуры Transformer.\n",
        "\n",
        "### Библиотеки\n",
        "\n",
        "Основные библиотеки:\n",
        " - [PyTorch](https://pytorch.org/)\n",
        " - [Transformers](https://github.com/huggingface/transformers)\n",
        "\n",
        "### Данные\n",
        "\n",
        "Данные лежат в архиве, который состоит из:\n",
        "\n",
        "- *train.tsv* - обучающая выборка. В каждой строке записаны: <слово / токен>, <тэг слова / токена>\n",
        "\n",
        "- *valid.tsv* - валидационная выборка, которую можно использовать для подбора гиперпарамеров и замеров качества. Имеет идентичную с train.tsv структуру.\n",
        "\n",
        "- *test.tsv* - тестовая выборка, по которой оценивается итоговое качество. Имеет идентичную с train.tsv структуру.\n",
        "\n",
        "Скачать данные можно здесь: [ссылка](https://github.com/dayyass/msu_task_3_ner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5BCB1EfQan1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96f71851-c6b8-458a-92ba-ebd393beadba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.21.6\n",
            "  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.0.2\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.9.0\n",
            "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.12.1\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.0\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.21.1\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (1.59.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.9.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (3.5.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (67.7.2)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.9.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.9.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (3.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.0) (0.41.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.1) (0.19.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.21.1) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.21.1)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.0) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.0) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.1) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.9.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.9.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.9.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.9.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.0) (3.2.2)\n",
            "Installing collected packages: tokenizers, tensorboard-plugin-wit, tqdm, torch, tensorboard-data-server, numpy, transformers, scikit-learn, google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.13.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.0.2 which is incompatible.\n",
            "flax 0.7.5 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "jax 0.4.20 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "jaxlib 0.4.20+cuda11.cudnn86 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.21.6 which is incompatible.\n",
            "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.9.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 numpy-1.21.6 scikit-learn-1.0.2 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.12.1 torch-1.12.1 tqdm-4.64.0 transformers-4.21.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        " !pip install numpy==1.21.6 scikit-learn==1.0.2 tensorboard==2.9.0 torch==1.12.1 tqdm==4.64.0 transformers==4.21.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thidpb9qQELS"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import Counter, defaultdict, namedtuple\n",
        "from typing import Tuple, List, Dict, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiDlmbY2QELT"
      },
      "source": [
        "Зафиксируем seed для воспроизводимости результатов (желательно делать **всегда**!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt3ISg3aQELU"
      },
      "outputs": [],
      "source": [
        "def set_global_seed(seed: int) -> None:\n",
        "    \"\"\"\n",
        "    Set global seed for reproducibility.\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "set_global_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhIg0ZBzQELV"
      },
      "source": [
        "Проинициализируем device (CPU / GPU) на котором будем работать (желательно **GPU**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rboLOv95QELV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "1a642a56-0ab5-44e2-eac8-7850ace0f902"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW16ryFIQELW"
      },
      "source": [
        "Здесь и далее проинициализируем *tensorboard* для логгирования метрики в процессе обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O7Y8hReTODp",
        "outputId": "96e8ee4b-7eb6-4d6d-8d0d-17ee93363153"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 5238), started 0:34:58 ago. (Use '!kill 5238' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k3Nhd3IQELY"
      },
      "source": [
        "## Часть 1. Подготовка данных (4 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qYjOMuPQELY"
      },
      "source": [
        "Первым делом нам нужно считать данные. Давайте напишем функцию, которая на вход принимает путь до одного из conll-2003 файла и возвращает два списка:\n",
        "- список списков слов / токенов (и соответствующий ему)\n",
        "- список списков тегов\n",
        "\n",
        "P.S. Сделаем данную функцию более гибкой, подавая на вход еще булеву переменную, считываем ли мы данные в *lowercase* или нет.\n",
        "\n",
        "**Задание. Реализуйте функцию read_conll2003.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQdCfX2OQELZ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "def read_conll2003(\n",
        "    path: str,\n",
        "    lower: bool = True,\n",
        ") -> Tuple[List[List[str]], List[List[str]]]:\n",
        "    token_seq = []\n",
        "    label_seq = []\n",
        "    x = []\n",
        "    y = []\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        tsv_file = csv.reader(f, delimiter=\"\\t\", quoting=3)\n",
        "        for l in tsv_file:\n",
        "\n",
        "            if (len(l)==2):\n",
        "                if (lower):\n",
        "                    l[0]=l[0].lower()\n",
        "                x.append(l[0])\n",
        "                y.append(l[1])\n",
        "            else:\n",
        "                token_seq.append(x)\n",
        "                label_seq.append(y)\n",
        "                x = []\n",
        "                y = []\n",
        "\n",
        "    return token_seq, label_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYm8xEvFQELb"
      },
      "source": [
        "Считаем все три файла:\n",
        "- *train.tsv*\n",
        "- *valid.tsv*\n",
        "- *test.tsv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-inr1BPgQELb"
      },
      "outputs": [],
      "source": [
        "train_token_seq, train_label_seq = read_conll2003(\"data/train.tsv\")\n",
        "valid_token_seq, valid_label_seq = read_conll2003(\"data/valid.tsv\")\n",
        "test_token_seq, test_label_seq = read_conll2003(\"data/test.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoNc1VUQELc"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HK8AcwWGQELd",
        "outputId": "d949fff7-3fb9-4e3e-c2fa-853d449f8314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eu\tB-ORG\n",
            "rejects\tO\n",
            "german\tB-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tO\n",
            "british\tB-MISC\n",
            "lamb\tO\n",
            ".\tO\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(train_token_seq[0], train_label_seq[0]):\n",
        "    print(f\"{token}\\t{label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8SqDeMJjF3Y",
        "outputId": "4939b43d-4fe6-4125-a383-6cb89b573d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cricket\tO\n",
            "-\tO\n",
            "leicestershire\tB-ORG\n",
            "take\tO\n",
            "over\tO\n",
            "at\tO\n",
            "top\tO\n",
            "after\tO\n",
            "innings\tO\n",
            "victory\tO\n",
            ".\tO\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(valid_token_seq[0], valid_label_seq[0]):\n",
        "    print(f\"{token}\\t{label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddFE7p5kjF_p",
        "outputId": "b000a8f1-49b9-4022-905e-78077ee2e666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "soccer\tO\n",
            "-\tO\n",
            "japan\tB-LOC\n",
            "get\tO\n",
            "lucky\tO\n",
            "win\tO\n",
            ",\tO\n",
            "china\tB-PER\n",
            "in\tO\n",
            "surprise\tO\n",
            "defeat\tO\n",
            ".\tO\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(test_token_seq[0], test_label_seq[0]):\n",
        "    print(f\"{token}\\t{label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ4Go3IXfDit",
        "outputId": "f5d3dc81-6aa2-4311-8168-b0c9dc0976d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(train_token_seq) == len(train_label_seq), \"Длины тренировочных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
        "assert len(valid_token_seq) == len(valid_label_seq), \"Длины валидационных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
        "assert len(test_token_seq) == len(test_label_seq), \"Длины тестовых token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
        "\n",
        "assert train_token_seq[0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Ошибка в тренировочном token_seq\"\n",
        "assert train_label_seq[0] == ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], \"Ошибка в тренировочном label_seq\"\n",
        "\n",
        "assert valid_token_seq[0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Ошибка в валидационном token_seq\"\n",
        "assert valid_label_seq[0] == ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], \"Ошибка в валидационном label_seq\"\n",
        "\n",
        "assert test_token_seq[0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Ошибка в тестовом token_seq\"\n",
        "assert test_label_seq[0] == ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], \"Ошибка в тестовом label_seq\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j96zKo6PQELd"
      },
      "source": [
        "Датасет CoNLL-2003 представлен в виде разметки **BIO**, где лейбл:\n",
        "- *B-{label}* - начало сущности *{label}*\n",
        "- *I-{label}* - продолжение сущности *{label}*\n",
        "- *O* - отсутсвие сущности\n",
        "\n",
        "Также существует другие разметки последовательностей, например **BILUO**. Подробнее с разметками можно ознакомится во вспомогательном ноутбуке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SVL4USbQELe"
      },
      "source": [
        "### Подготовка словарей\n",
        "\n",
        "Чтобы обучать нейронную сеть, мы будем использовать два отображения:\n",
        "- {**token**}→{**token_idx**}: соответствие между словом / токеном и строкой в *embedding* матрице (начинается с 0);\n",
        "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
        "\n",
        "Теперь нам необходимо реализовать две функции:\n",
        "- get_token2idx\n",
        "- get_label2idx\n",
        "\n",
        "которые будут возвращать соответствующие словари.\n",
        "\n",
        "P.S. token2idx словарь должен также содержать специальные токены:\n",
        "- `<PAD>` - спецтокен для паддинга, так как мы собираемся обучать модели батчами\n",
        "- `<UNK>` - спецтокен для обработки слов / токенов, которых нет в словаре (актуально для инференса)\n",
        "\n",
        "Давайте для удобства дадим им idx 0 и 1 соответственно.\n",
        "\n",
        "P.P.S. В get_token2idx можно также добавить параметр *min_count*, который будет включать только слова превышающие определенную частоту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOnc3UHpQELf"
      },
      "source": [
        "Сначала соберем:\n",
        "- token2cnt - словарь из уникального слова / токена в количество это слова / токена в тренировочной выборке (важно, что только в тренировочной!)\n",
        "- label_set - список из уникальных тегов\n",
        "\n",
        "P.S. Также можно использовать стемминг для того, чтобы преобразовывать разные словоформы одного слова в один токен, но мы опустим этот момент.\n",
        "\n",
        "**Задание. Реализуйте функции get_token2idx и get_label2idx.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IthnXKsoo7A3"
      },
      "outputs": [],
      "source": [
        "token2cnt = Counter([token for sentence in train_token_seq for token in sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_v8YUM7QELg",
        "outputId": "77f22cb2-a640-43e5-996b-fcaa2020ce99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('the', 8390),\n",
              " ('.', 7374),\n",
              " (',', 7290),\n",
              " ('of', 3815),\n",
              " ('in', 3621),\n",
              " ('to', 3424),\n",
              " ('a', 3199),\n",
              " ('and', 2872),\n",
              " ('(', 2861),\n",
              " (')', 2861)]"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token2cnt.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSm7B546nmDh",
        "outputId": "d6863a06-52a2-42da-859f-41097e761673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество уникальных слов в тренировочном датасете: 21010\n",
            "Количество слов встречающихся только один раз в тренировочном датасете: 10060\n"
          ]
        }
      ],
      "source": [
        "print(f\"Количество уникальных слов в тренировочном датасете: {len(token2cnt)}\")\n",
        "print(f\"Количество слов встречающихся только один раз в тренировочном датасете: {len([token for token, cnt in token2cnt.items() if cnt == 1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRtCHt1QruSU"
      },
      "source": [
        "Как мы видим, у нас есть много слов, которые встречаются только один раз в датасете. Очевидно, что выучиться по ним у нас не получиться, мы только переобучимся, поэтому давайте выкинем такие слова при формировании нашего словаря."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCaPftCyQELi"
      },
      "outputs": [],
      "source": [
        "# используйте параметр min_count для того, чтобы отсекать слова частотой cnt < min_count\n",
        "\n",
        "def get_token2idx(\n",
        "    token2cnt: Dict[str, int],\n",
        "    min_count: int,\n",
        ") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Get mapping from tokens to indices to use with Embedding layer.\n",
        "    \"\"\"\n",
        "\n",
        "    token2idx: Dict[str, int] = {'<PAD>': 0, '<UNK>':1}\n",
        "\n",
        "    token2cnt = {x: token2cnt[x] for x in token2cnt if token2cnt[x] >= min_count}\n",
        "    for index, word in enumerate(token2cnt.keys()):\n",
        "        token2idx[word] = index+2\n",
        "\n",
        "    return token2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFK130y-sLH4"
      },
      "outputs": [],
      "source": [
        "token2idx = get_token2idx(token2cnt, min_count=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g69HFZC7QELh"
      },
      "outputs": [],
      "source": [
        "# Функция для сортировки тегов, чтобы сначала был тег O, потом теги B- и только после теги I- (можно задать вручную)\n",
        "\n",
        "def sort_labels_func(x: str) -> int:\n",
        "    if x == \"O\":\n",
        "        return 0\n",
        "    elif x.startswith(\"B-\"):\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "label_set = sorted(\n",
        "    set(label for sentence in train_label_seq for label in sentence),\n",
        "    key=lambda x: (sort_labels_func(x), x),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI_3m4qbQELi",
        "outputId": "116bfcab-c56b-4ad9-b0f8-894ead418318"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O', 'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER']"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6i51GPtQELj"
      },
      "outputs": [],
      "source": [
        "def get_label2idx(label_set: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Get mapping from labels to indices.\n",
        "    \"\"\"\n",
        "\n",
        "    label2idx: Dict[str, int] = {}\n",
        "    # YOUR CODE HERE\n",
        "    for index, word  in enumerate(label_set):\n",
        "        label2idx[word] = index\n",
        "\n",
        "    return label2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW6fK0HtQELk"
      },
      "outputs": [],
      "source": [
        "label2idx = get_label2idx(label_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U13l-2IOQELk"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7U7bMrHQELl",
        "outputId": "53d80294-e80b-4536-acfa-99bf171d2e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<PAD>\t0\n",
            "<UNK>\t1\n",
            "eu\t2\n",
            "german\t3\n",
            "call\t4\n",
            "to\t5\n",
            "boycott\t6\n",
            "british\t7\n",
            "lamb\t8\n",
            ".\t9\n"
          ]
        }
      ],
      "source": [
        "for token, idx in list(token2idx.items())[:10]:\n",
        "    print(f\"{token}\\t{idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp75V-o2QELl",
        "outputId": "7f85dca1-eea6-4789-e261-1552bcaf91d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O\t0\n",
            "B-LOC\t1\n",
            "B-MISC\t2\n",
            "B-ORG\t3\n",
            "B-PER\t4\n",
            "I-LOC\t5\n",
            "I-MISC\t6\n",
            "I-ORG\t7\n",
            "I-PER\t8\n"
          ]
        }
      ],
      "source": [
        "for label, idx in label2idx.items():\n",
        "    print(f\"{label}\\t{idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYb4BdAUhNzk",
        "outputId": "dc62e8d8-7b62-4638-c436-aaf657dc5287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(get_token2idx(token2cnt, min_count=1)) == 21012, \"Ошибка в длине словаря, скорее всего неверно реализован min_count\"\n",
        "assert len(token2idx) == 10952, \"Неправильная длина token2idx, скорее всего неверно реализован min_count\"\n",
        "assert len(label2idx) == 9, \"Неправильная длина label2idx\"\n",
        "\n",
        "assert list(token2idx.items())[:10] == [('<PAD>', 0), ('<UNK>', 1), ('eu', 2), ('german', 3), ('call', 4), ('to', 5), ('boycott', 6), ('british', 7), ('lamb', 8), ('.', 9)], \"Неправильно сформированный token2idx\"\n",
        "assert label2idx == {'O': 0, 'B-LOC': 1, 'B-MISC': 2, 'B-ORG': 3, 'B-PER': 4, 'I-LOC': 5, 'I-MISC': 6, 'I-ORG': 7, 'I-PER': 8}, \"Неправильно сформированный label2idx\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItPs1DmOQELm"
      },
      "source": [
        "### Подготовка датасета и загрузчика\n",
        "\n",
        "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины.\n",
        "\n",
        "Из предыдущего практического задания вы должны знать о `Dataset`'е (`torch.utils.data.Dataset`) - структура данных, которая хранит и может по индексу отдавать данные для обучения. Датасет должен наследоваться от стандартного PyTorch класса Dataset и переопределять методы `__len__` и `__getitem__`.\n",
        "\n",
        "Метод `__getitem__` должен возвращать индексированную последовательность и её теги.\n",
        "\n",
        "**Не забудьте** про `<UNK>` спецтокен для неизвестных слов!\n",
        "    \n",
        "Давайте напишем кастомный датасет под нашу задачу, который на вход (метод `__init__`) будет принимать:\n",
        "- token_seq - список списков слов / токенов\n",
        "- label_seq - список списков тегов\n",
        "- token2idx\n",
        "- label2idx\n",
        "\n",
        "и возвращать из метода `__getitem__` два int64 тензора (`torch.LongTensor`) из индексов слов / токенов в сэмпле и индексов соответвующих тегов:\n",
        "\n",
        "**Задание. Реализуйте класс датасета NERDataset.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdZvnUUpQELm"
      },
      "outputs": [],
      "source": [
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_seq: List[List[str]],\n",
        "        label_seq: List[List[str]],\n",
        "        token2idx: Dict[str, int],\n",
        "        label2idx: Dict[str, int],\n",
        "    ):\n",
        "        self.token2idx = token2idx\n",
        "        self.label2idx = label2idx\n",
        "\n",
        "        self.token_seq = [self.process_tokens(tokens, token2idx) for tokens in token_seq]\n",
        "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_seq)\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        # YOUR CODE HERE\n",
        "        return [self.token_seq[idx], self.label_seq[idx] ]\n",
        "\n",
        "    @staticmethod\n",
        "    def process_tokens(\n",
        "        tokens: List[str],\n",
        "        token2idx: Dict[str, int],\n",
        "        unk: str = \"<UNK>\",\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Transform list of tokens into list of tokens' indices.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        ans = []\n",
        "        for token in tokens:\n",
        "            if token in token2idx:\n",
        "                idx = token2idx[token]\n",
        "            else:\n",
        "                idx = token2idx[unk]\n",
        "            ans.append(idx)\n",
        "        return torch.Tensor(ans)\n",
        "\n",
        "    @staticmethod\n",
        "    def process_labels(\n",
        "        labels: List[str],\n",
        "        label2idx: Dict[str, int],\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Transform list of labels into list of labels' indices.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        ans = []\n",
        "        for label in labels:\n",
        "            idx = label2idx[label]\n",
        "            ans.append(idx)\n",
        "        return torch.Tensor(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCvaPJERQELn"
      },
      "source": [
        "Создадим три датасета:\n",
        "- *train_dataset*\n",
        "- *valid_dataset*\n",
        "- *test_dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUMsSNkoQELn"
      },
      "outputs": [],
      "source": [
        "train_dataset = NERDataset(\n",
        "    token_seq=train_token_seq,\n",
        "    label_seq=train_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")\n",
        "valid_dataset = NERDataset(\n",
        "    token_seq=valid_token_seq,\n",
        "    label_seq=valid_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")\n",
        "test_dataset = NERDataset(\n",
        "    token_seq=test_token_seq,\n",
        "    label_seq=test_label_seq,\n",
        "    token2idx=token2idx,\n",
        "    label2idx=label2idx,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQIq1pAWQELo"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Scync0QELo",
        "outputId": "1669dbf8-2b23-40e0-9012-430cd44c9bc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([2., 1., 3., 4., 5., 6., 7., 8., 9.]),\n",
              " tensor([3., 0., 2., 0., 0., 0., 2., 0., 0.])]"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyAazaLzjQ-K",
        "outputId": "ed864972-9051-4b85-e206-30e59d9971c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([1737.,  571., 1777.,  197.,  687.,  145.,  349.,  111., 1819., 1558.,\n",
              "            9.]),\n",
              " tensor([0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.])]"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHuuh3YmjRNt",
        "outputId": "5f214ff3-7bbb-4b7c-8c9c-a861562b09cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([1516.,  571., 1434., 1729., 4893., 2014.,   67.,  310.,  215., 3157.,\n",
              "         3139.,    9.]),\n",
              " tensor([0., 0., 1., 0., 0., 0., 0., 4., 0., 0., 0., 0.])]"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gox6uyF2idwZ",
        "outputId": "8abb0480-9913-4f8a-d50e-1713611cbc9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
        "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
        "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
        "\n",
        "assert torch.equal(train_dataset[0][0], torch.tensor([2,1,3,4,5,6,7,8,9])), \"Неправильно сформированный train_dataset\"\n",
        "assert torch.equal(train_dataset[0][1], torch.tensor([3,0,2,0,0,0,2,0,0])), \"Неправильно сформированный train_dataset\"\n",
        "\n",
        "assert torch.equal(valid_dataset[0][0], torch.tensor([1737,571,1777,197,687,145,349,111,1819,1558,9])), \"Неправильно сформированный valid_dataset\"\n",
        "assert torch.equal(valid_dataset[0][1], torch.tensor([0,0,3,0,0,0,0,0,0,0,0])), \"Неправильно сформированный valid_dataset\"\n",
        "\n",
        "assert torch.equal(test_dataset[0][0], torch.tensor([1516,571,1434,1729,4893,2014,67,310,215,3157,3139,9])), \"Неправильно сформированный test_dataset\"\n",
        "assert torch.equal(test_dataset[0][1], torch.tensor([0,0,1,0,0,0,0,4,0,0,0,0])), \"Неправильно сформированный test_dataset\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWjJuAk7QELp"
      },
      "source": [
        "Для того, чтобы дополнять последовательности паддингом, будем использовать параметр `collate_fn` класса `DataLoader`.\n",
        "\n",
        "Принимая последовательность пар тензоров для предложений и тегов, необходимо дополнить все последовательности до последовательности максимальной длины в батче.\n",
        "\n",
        "Используйте для дополнения спецтокен `<PAD>` для последовательностей слов / токенов и -1 для последовательностей тегов.\n",
        "\n",
        "**hint**: удобно использовать метод **torch.nn.utils.rnn**. Обратите особое внимание на параметр *batch_first*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZiJVM5qQELp"
      },
      "source": [
        "`Collator` можно реализовать двумя способами:\n",
        "- класс с методом `__call__`\n",
        "- функцию\n",
        "\n",
        "Мы пойдем первым путем.\n",
        "\n",
        "Инициализировать экземпляр класса `Collator` (метод `__init__`) с помощью двух параметров:\n",
        "- id `<PAD>` спецтокена для последовательностей слов / токенов\n",
        "- id `<PAD>` спецтокена для последовательностей тегов (значение -1)\n",
        "\n",
        "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
        "\n",
        "На выходе мы хотим получить два тензора:\n",
        "- западденные индексы слов / токенов\n",
        "- западденные индексы тегов\n",
        "    \n",
        "P.S. `<PAD>` значение нужно для того, чтобы при подсчете лосса легко отличать западдированные токены от других. Можно использовать параметр *ignore_index* при инициализации лосса.\n",
        "\n",
        "**Задание. Реализуйте класс коллатора NERCollator.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNHNwoLnQELp"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class NERCollator:\n",
        "    \"\"\"\n",
        "    Collator that handles variable-size sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_padding_value: int,\n",
        "        label_padding_value: int,\n",
        "    ):\n",
        "        self.token_padding_value = token_padding_value\n",
        "        self.label_padding_value = label_padding_value\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        batch: List[Tuple[torch.LongTensor, torch.LongTensor]],\n",
        "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "\n",
        "        tokens, labels = zip(*batch)\n",
        "        labels = pad_sequence(labels, padding_value= self.label_padding_value, batch_first = True).long()\n",
        "        tokens = pad_sequence(tokens, padding_value= self.token_padding_value, batch_first = True).long()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        return tokens, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZUMwVQTQELq"
      },
      "outputs": [],
      "source": [
        "collator = NERCollator(\n",
        "    token_padding_value=token2idx[\"<PAD>\"],\n",
        "    label_padding_value=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsgfij8WQELq"
      },
      "source": [
        "Теперь всё готово, чтобы задать `DataLoader`'ы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFljkiBOQELr"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i34wGJ4uQELr"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLlr_DztQELr"
      },
      "outputs": [],
      "source": [
        "tokens, labels = next(iter(train_dataloader))\n",
        "\n",
        "tokens = tokens.to(device)\n",
        "labels = labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdMMEDdbQELs",
        "outputId": "b7bd1bb5-248b-482c-cd5e-c98c6c3709f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[7796, 1162, 2553, 7237, 1342,    0,    0,    0,    0,    0],\n",
              "        [ 125, 1167,    1,   67, 1349,  489, 1215, 1364, 1365, 1366]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w--fhADKQELs",
        "outputId": "708d1cec-7e0c-4f07-f723-900aab50203c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 3,  0,  3,  7,  0, -1, -1, -1, -1, -1],\n",
              "        [ 0,  4,  8,  0,  1,  0,  0,  0,  0,  0]], device='cuda:0')"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFeX0AYKlhGk",
        "outputId": "3c5a0193-2caf-4d7c-f02f-272b91e0f6f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "train_tokens, train_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(train_tokens, torch.tensor([[ 2,  1,  3,  4,  5,  6,  7,  8,  9], [10, 11,  0,  0,  0,  0,  0,  0,  0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(train_labels, torch.tensor([[ 3,  0,  2,  0,  0,  0,  2,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "valid_tokens, valid_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(valid_tokens, torch.tensor([[ 1737,   571,  1777,   197,   687,   145,   349,   111,  1819,  1558, 9], [  248, 10679,     0,     0,     0,     0,     0,     0,     0,     0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(valid_labels, torch.tensor([[ 0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0], [ 1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "test_tokens, test_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(test_tokens, torch.tensor([[1516,  571, 1434, 1729, 4893, 2014,   67,  310,  215, 3157, 3139,    9], [   1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(test_labels, torch.tensor([[ 0,  0,  1,  0,  0,  0,  0,  4,  0,  0,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ul5gLriQELs"
      },
      "source": [
        "## Часть 2. BiLSTM-теггер (6 баллов)\n",
        "\n",
        "Определите архитектуру сети, используя библиотеку PyTorch.\n",
        "\n",
        "Ваша архитектура в этом пункте должна соответствовать стандартному теггеру:\n",
        "* Embedding слой на входе\n",
        "* LSTM (однонаправленный или двунаправленный)слой для обработки последовательности\n",
        "* Dropout (заданный отдельно или встроенный в LSTM) для уменьшения переобучения\n",
        "* Linear слой на выходе\n",
        "\n",
        "Для обучения сети используйте поэлементную кросс-энтропийную функцию потерь.\n",
        "\n",
        "**Обратите внимание**, что `<PAD>` токены не должны учавствовать в подсчёте функции потерь. В качестве оптимизатора рекомендуется использовать Adam. Для получения значений предсказаний по выходам модели используйте функцию `argmax`.\n",
        "\n",
        "**Задание. Реализуйте класс модели BiLSTM.** **<font color='red'>(2 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMiLQljZQELt"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_embeddings: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "        dropout_rnn: float,\n",
        "        bidirectional: bool,\n",
        "        n_classes: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        #torch.nn.init.uniform_(self.embedding.weight)\n",
        "        #self.embedding.weight.requires_grad = False\n",
        "        self.input_dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=bidirectional, batch_first=True,dropout=dropout_rnn)\n",
        "        self.output_dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.head = torch.nn.Linear(self.hidden_size*2 , n_classes, bias=True)\n",
        "\n",
        "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
        "        embed = self.embedding(tokens)\n",
        "        embed = self.input_dropout(embed)\n",
        "\n",
        "        # используем специальную функцию pack_padded_sequence для того, чтобы получить структуру PackedSequence\n",
        "        # которая не учитывать паддинг при проходе rnn\n",
        "        length = (tokens != 0).sum(dim=1).detach().cpu()\n",
        "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "            embed, length, batch_first=True, enforce_sorted=False\n",
        "          )\n",
        "\n",
        "        # используем специальную функцию pad_packed_sequence для того, чтобы получить тензор из PackedSequence\n",
        "        packed_rnn_output, _ = self.rnn(packed_embed)\n",
        "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
        "            packed_rnn_output, batch_first=True)\n",
        "        rnn_output = self.output_dropout(rnn_output)\n",
        "\n",
        "\n",
        "        logits = self.head(rnn_output)\n",
        "        return logits.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zps8HL2VQELu"
      },
      "outputs": [],
      "source": [
        "model = BiLSTM(\n",
        "    num_embeddings=len(token2idx),\n",
        "    embedding_dim=100,\n",
        "    hidden_size=100,\n",
        "    num_layers=1,\n",
        "    dropout=0.0,\n",
        "    dropout_rnn=0.0,\n",
        "    bidirectional=True,\n",
        "    n_classes=len(label2idx),\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmg2_C_oQELu",
        "outputId": "cc700776-3bbe-43a3-d0db-049ad74965e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (embedding): Embedding(10952, 100)\n",
              "  (input_dropout): Dropout(p=0.0, inplace=False)\n",
              "  (rnn): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
              "  (output_dropout): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=200, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDvWB5J2QELv"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn5Pu1UKQELv"
      },
      "outputs": [],
      "source": [
        "\n",
        "outputs = model(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02Bsh8eQELw",
        "outputId": "6b704a7d-2cc9-49b0-a260-d4f6f52d2405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 9, 10])\n",
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "print(outputs.shape)\n",
        "assert outputs.shape == torch.Size([2, 9, 10])\n",
        "assert 2 < criterion(outputs, labels) < 3\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjhkK9QFQELu"
      },
      "source": [
        "### Эксперименты\n",
        "\n",
        "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше 0.76.\n",
        "\n",
        "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4hdrFZ9iRPi"
      },
      "outputs": [],
      "source": [
        "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir=f\"logs/BiLSTMModel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruMTBSkQQELx"
      },
      "source": [
        "**Задание. Реализуйте функцию подсчета метрик compute_metrics.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkpo3JgWQELx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    outputs: torch.Tensor,\n",
        "    labels: torch.LongTensor,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute NER metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Не забудюте отфильтровать <PAD> токен\n",
        "\n",
        "    labels = labels.flatten().cpu()\n",
        "    y_true = labels[torch.where(labels != -1)]\n",
        "\n",
        "    _, outputs = torch.max(outputs, dim = 1)\n",
        "    y_pred = outputs.flatten().cpu()\n",
        "    y_pred = y_pred[torch.where(labels != -1)]\n",
        "\n",
        "    # accuracy\n",
        "    accuracy = accuracy_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "    )\n",
        "\n",
        "    # precision\n",
        "    precision_micro = precision_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"micro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    precision_macro = precision_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"macro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    precision_weighted = precision_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"weighted\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "    # recall\n",
        "    recall_micro = recall_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"micro\",\n",
        "        zero_division=0,\n",
        "\n",
        "    )\n",
        "    recall_macro = recall_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"macro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    recall_weighted = recall_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"weighted\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "    # f1\n",
        "    f1_micro = f1_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"micro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    f1_macro = f1_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"macro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    f1_weighted = f1_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"weighted\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "    metrics[\"accuracy\"] = accuracy\n",
        "\n",
        "    metrics[\"precision_micro\"]    = precision_micro\n",
        "    metrics[\"precision_macro\"]    = precision_macro\n",
        "    metrics[\"precision_weighted\"] = precision_weighted\n",
        "\n",
        "    metrics[\"recall_micro\"]    = recall_micro\n",
        "    metrics[\"recall_macro\"]    = recall_macro\n",
        "    metrics[\"recall_weighted\"] = recall_weighted\n",
        "\n",
        "    metrics[\"f1_micro\"]    = f1_micro\n",
        "    metrics[\"f1_macro\"]    = f1_macro\n",
        "    metrics[\"f1_weighted\"] = f1_weighted\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzj89UygQEL0"
      },
      "source": [
        "**Задание. Реализуйте функции обучения и тестирования train_epoch и evaluate_epoch.** **<font color='red'>(2 балла)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG3vQbc_QEL0"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One training cycle (loop).\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    for i, (tokens, labels) in tqdm(\n",
        "        enumerate(dataloader),\n",
        "        total=len(dataloader),\n",
        "        desc=\"loop over train batches\",\n",
        "    ):\n",
        "\n",
        "        tokens, labels = tokens.to(device), labels.to(device)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        # Подсчет лосса и шаг оптимизатора\n",
        "        model.zero_grad()\n",
        "\n",
        "        tag_scores = model(tokens)\n",
        "        loss = criterion(tag_scores, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "        writer.add_scalar(\n",
        "            \"batch loss / train\", loss.item(), epoch * len(dataloader) + i\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            outputs_inference = model(tokens)\n",
        "            model.train()\n",
        "\n",
        "        batch_metrics = compute_metrics(\n",
        "            outputs=outputs_inference,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        for metric_name, metric_value in batch_metrics.items():\n",
        "            batch_metrics_list[metric_name].append(metric_value)\n",
        "            writer.add_scalar(\n",
        "                f\"batch {metric_name} / train\",\n",
        "                metric_value,\n",
        "                epoch * len(dataloader) + i,\n",
        "            )\n",
        "\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    print(f\"Train loss: {avg_loss}\\n\")\n",
        "    writer.add_scalar(\"loss / train\", avg_loss, epoch)\n",
        "\n",
        "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "        metric_value = np.mean(metric_value_list)\n",
        "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
        "        writer.add_scalar(f\"{metric_name} / train\", metric_value, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4ztFogtQEL0"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One evaluation cycle (loop).\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (tokens, labels) in tqdm(\n",
        "            enumerate(dataloader),\n",
        "            total=len(dataloader),\n",
        "            desc=\"loop over test batches\",\n",
        "        ):\n",
        "\n",
        "            tokens, labels = tokens.to(device), labels.to(device)\n",
        "\n",
        "            # YOUR CODE HERE\n",
        "            # Подсчет лосса\n",
        "            outputs = model(tokens)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            writer.add_scalar(\n",
        "                \"batch loss / test\", loss.item(), epoch * len(dataloader) + i\n",
        "            )\n",
        "\n",
        "            batch_metrics = compute_metrics(\n",
        "                outputs=outputs,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            for metric_name, metric_value in batch_metrics.items():\n",
        "                batch_metrics_list[metric_name].append(metric_value)\n",
        "                writer.add_scalar(\n",
        "                    f\"batch {metric_name} / test\",\n",
        "                    metric_value,\n",
        "                    epoch * len(dataloader) + i,\n",
        "                )\n",
        "\n",
        "        avg_loss = np.mean(epoch_loss)\n",
        "        print(f\"Test loss:  {avg_loss}\\n\")\n",
        "        writer.add_scalar(\"loss / test\", avg_loss, epoch)\n",
        "\n",
        "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "            metric_value = np.mean(metric_value_list)\n",
        "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
        "            writer.add_scalar(f\"{metric_name} / test\", np.mean(metric_value), epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7Z5MTNzQEL1"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    n_epochs: int,\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    test_dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Training loop.\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
        "\n",
        "        train_epoch(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "        )\n",
        "        evaluate_epoch(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTxfU0BfQEL1"
      },
      "source": [
        "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOgEJsyoJ0dC",
        "outputId": "cf0d55b9-e74d-47ae-d03d-e63ff4250b54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 5238), started 0:41:43 ago. (Use '!kill 5238' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz6mjGZUQEL2",
        "outputId": "f892aa8a-197a-4bc3-ce76-e92fe62ef9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1 / 300]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|█████████████| 7493/7493 [00:47<00:00, 158.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.7162202343275209\n",
            "\n",
            "Train accuracy: 0.8176733508528256\n",
            "\n",
            "Train precision_micro: 0.8176733508528256\n",
            "\n",
            "Train precision_macro: 0.36326264703874805\n",
            "\n",
            "Train precision_weighted: 0.7124099645124161\n",
            "\n",
            "Train recall_micro: 0.8176733508528256\n",
            "\n",
            "Train recall_macro: 0.39418130655413613\n",
            "\n",
            "Train recall_weighted: 0.8176733508528256\n",
            "\n",
            "Train f1_micro: 0.8176733508528256\n",
            "\n",
            "Train f1_macro: 0.3709149633193839\n",
            "\n",
            "Train f1_weighted: 0.753545331917927\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████████| 3465/3465 [00:13<00:00, 256.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.5515611142569193\n",
            "\n",
            "Test accuracy: 0.8484862802261407\n",
            "\n",
            "Test precision_micro: 0.8484862802261407\n",
            "\n",
            "Test precision_macro: 0.5873741642628185\n",
            "\n",
            "Test precision_weighted: 0.7806537956704694\n",
            "\n",
            "Test recall_micro: 0.8484862802261407\n",
            "\n",
            "Test recall_macro: 0.6169040982042243\n",
            "\n",
            "Test recall_weighted: 0.8484862802261407\n",
            "\n",
            "Test f1_micro: 0.8484862802261407\n",
            "\n",
            "Test f1_macro: 0.5939688997802355\n",
            "\n",
            "Test f1_weighted: 0.8049093361256343\n",
            "\n",
            "Epoch [2 / 300]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|█████████████| 7493/7493 [00:46<00:00, 162.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.4435079256364184\n",
            "\n",
            "Train accuracy: 0.876660841544516\n",
            "\n",
            "Train precision_micro: 0.876660841544516\n",
            "\n",
            "Train precision_macro: 0.5455114702939275\n",
            "\n",
            "Train precision_weighted: 0.819441102095202\n",
            "\n",
            "Train recall_micro: 0.876660841544516\n",
            "\n",
            "Train recall_macro: 0.5444911130007679\n",
            "\n",
            "Train recall_weighted: 0.876660841544516\n",
            "\n",
            "Train f1_micro: 0.8766608415445161\n",
            "\n",
            "Train f1_macro: 0.53373932375562\n",
            "\n",
            "Train f1_weighted: 0.839749948746517\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████████| 3465/3465 [00:13<00:00, 248.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.40679555007096646\n",
            "\n",
            "Test accuracy: 0.8868973988882053\n",
            "\n",
            "Test precision_micro: 0.8868973988882053\n",
            "\n",
            "Test precision_macro: 0.6789348395503537\n",
            "\n",
            "Test precision_weighted: 0.8500878402756635\n",
            "\n",
            "Test recall_micro: 0.8868973988882053\n",
            "\n",
            "Test recall_macro: 0.6874422962054177\n",
            "\n",
            "Test recall_weighted: 0.8868973988882053\n",
            "\n",
            "Test f1_micro: 0.8868973988882053\n",
            "\n",
            "Test f1_macro: 0.6759691519989974\n",
            "\n",
            "Test f1_weighted: 0.8618642296281379\n",
            "\n",
            "Epoch [3 / 300]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|█████████████| 7493/7493 [00:46<00:00, 161.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.3278582219041504\n",
            "\n",
            "Train accuracy: 0.9065539326816813\n",
            "\n",
            "Train precision_micro: 0.9065539326816813\n",
            "\n",
            "Train precision_macro: 0.647197341544398\n",
            "\n",
            "Train precision_weighted: 0.8721091598951632\n",
            "\n",
            "Train recall_micro: 0.9065539326816813\n",
            "\n",
            "Train recall_macro: 0.6384013376363432\n",
            "\n",
            "Train recall_weighted: 0.9065539326816813\n",
            "\n",
            "Train f1_micro: 0.9065539326816813\n",
            "\n",
            "Train f1_macro: 0.6318657954123198\n",
            "\n",
            "Train f1_weighted: 0.8829624948857366\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches:  77%|██████████▊   | 2669/3465 [00:09<00:02, 281.15it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m \n",
            "Cell \u001b[0;32mIn [146], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epochs, model, train_dataloader, test_dataloader, optimizer, criterion, writer, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m train_epoch(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[43mevaluate_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn [145], line 20\u001b[0m, in \u001b[0;36mevaluate_epoch\u001b[0;34m(model, dataloader, criterion, writer, device, epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m batch_metrics_list \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (tokens, labels) \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(dataloader),\n\u001b[1;32m     22\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader),\n\u001b[1;32m     23\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloop over test batches\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     ):\n\u001b[1;32m     26\u001b[0m         tokens, labels \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Подсчет лосса\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/profiler.py:485\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[0;34m(self, name, args)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "train(\n",
        "    n_epochs = 300,\n",
        "    model = model,\n",
        "    train_dataloader = train_dataloader,\n",
        "    test_dataloader = test_dataloader ,\n",
        "    optimizer = optimizer,\n",
        "    criterion = criterion,\n",
        "    writer = writer,\n",
        "    device = device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEbnLKPkJ0dD"
      },
      "outputs": [],
      "source": [
        "evaluate_epoch(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=0,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8R6nopyQEL-"
      },
      "source": [
        "## Часть 3. Transformers-теггер (6 баллов)\n",
        "\n",
        "В данной части задания нужно сделать все то же самое, но с использованием модели на базе архитектуры Transformer, а именно предлагается дообучать предобученную модель **BERT**.\n",
        "\n",
        "Для данной модели подразумевается специальная подготовка данных, с чего мы и начнем:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrbX5gFDQEL-"
      },
      "source": [
        "Модель **BERT** использует специальный токенизатор WordPiece для разбиения предложений на токены. Готовая предобученная версия такого токенизатора существует в библиотеке **transformers**. Есть два класса: `BertTokenizer` и `BertTokenizerFast`. Использовать можно любой, но второй вариант работает существенно быстрее.\n",
        "\n",
        "Токенизаторы можно обучать с нуля на своем корпусе данных, а можно подгружать уже готовые. Готовые токенизаторы, как правило, соответствуют предобученной конфигурации модели, которая использует словарь из этого токенизатора.\n",
        "\n",
        "Мы будем использовать базовую конфигурацию предобученного **BERT** для модели и токенизатора.\n",
        "\n",
        "P.S. Часто приходится проводить эксперименты с моделями разной архитектуры, например **BERT** и **GPT**, поэтому удобно использовать класс `AutoTokenizer`, который по названию модели сам определит, какой класс нужен для инициализации токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-UTiI4gQEL-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSbBhvnDQEMA"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilbert-base-cased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxWNX5i6QEMA"
      },
      "source": [
        "Подгружение предобученных моделей и токенизаторов в **huggingface** происходит с помощью конструктора **from_pretrained**.\n",
        "\n",
        "В данном конструкторе можно указать либо путь к предобученному токенизатору, либо название предобученной конфигурации, как в нашем случае: тогда **transformers** сам подгрузит нужные параметры:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tg_bCeaQEMA"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MIrbmNoQEMA"
      },
      "source": [
        "### Подготовка словарей\n",
        "\n",
        "В сравнении с рекуррентными моделями, на больше не нужно заниматься сборкой словаря, так как это уже сделано заранее благодаря токенизаторам и алгоритмам, стоящими за ними.\n",
        "\n",
        "Но нам как и прежде потребуется:\n",
        "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
        "\n",
        "Но данное отображение у нас уже реализовано в одной из предыдущих частей задания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvYF-4uaQEMB"
      },
      "source": [
        "### Подготовка датасета и загрузчика\n",
        "\n",
        "Мы также хотим обучать модель батчами, поэтому нам как и прежде понадобятся `Dataset`, `Collator` и `DataLoader`.\n",
        "\n",
        "Но мы не можем переиспользовать те, что в предыдущих частях задания, так как обработка данных должна производится немного иначе с использованием токенизатора.\n",
        "\n",
        "Давайте напишем новый кастомный датасет, который на вход (метод `__init__`) будет принимать:\n",
        "- token_seq - список списков слов / токенов\n",
        "- label_seq - список списков тегов\n",
        "\n",
        "и возвращать из метода `__getitem__` два списка:\n",
        "- список текстовых значений (`List[str]`) из индексов токенов в сэмпле\n",
        "- список целочисленных значений (`List[int]`) из индексов соответвующих тегов\n",
        "\n",
        "P.S. В отличие от предыдущего кастомного датасет, здесь мы возвращаем два `List`'а вместо `torch.LongTensor`, так как логику формирования западдированного батча мы перенесем в `Collator` из-за специфики работы токенизатора - он сам возвращает уже западдированный тензор с индексами токенов, а для индексов тегов нам нужно будет сделать это самостоятельно по аналогии с предыдущим датасетом.\n",
        "\n",
        "**Задание. Реализуйте класс датасета TransformersDataset.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EoNLDOOQEMB"
      },
      "outputs": [],
      "source": [
        "class TransformersDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Transformers Dataset for NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        token_seq: List[List[str]],\n",
        "        label_seq: List[List[str]],\n",
        "    ):\n",
        "        self.token_seq = token_seq\n",
        "        self.label_seq = [self.process_labels(labels, label2idx) for labels in label_seq]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_seq)\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "    ) -> Tuple[List[str], List[int]]:\n",
        "        tok = self.token_seq[idx]\n",
        "        seq = self.label_seq[idx]\n",
        "        return [tok, seq]\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    @staticmethod\n",
        "    def process_labels(\n",
        "        labels: List[str],\n",
        "        label2idx: Dict[str, int],\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Transform list of labels into list of labels' indices.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        ans = []\n",
        "        for label in labels:\n",
        "            idx = label2idx[label]\n",
        "            ans.append(idx)\n",
        "        return ans\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1oNc-31QEMB"
      },
      "source": [
        "Создадим три датасета:\n",
        "- *train_dataset*\n",
        "- *valid_dataset*\n",
        "- *test_dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqg56Jf8QEMC"
      },
      "outputs": [],
      "source": [
        "train_dataset = TransformersDataset(\n",
        "    token_seq=train_token_seq,\n",
        "    label_seq=train_label_seq,\n",
        ")\n",
        "valid_dataset = TransformersDataset(\n",
        "    token_seq=valid_token_seq,\n",
        "    label_seq=valid_label_seq,\n",
        ")\n",
        "test_dataset = TransformersDataset(\n",
        "    token_seq=test_token_seq,\n",
        "    label_seq=test_label_seq,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdIS6XrvQEMC"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT00Pjy6QEMC",
        "outputId": "6bb42a27-08f0-49f5-96f5-52573a1a75af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
              " [3, 0, 2, 0, 0, 0, 2, 0, 0]]"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYal2icQmuD-",
        "outputId": "572bab3f-d53d-46d2-dd6f-367042966062"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['cricket',\n",
              "  '-',\n",
              "  'leicestershire',\n",
              "  'take',\n",
              "  'over',\n",
              "  'at',\n",
              "  'top',\n",
              "  'after',\n",
              "  'innings',\n",
              "  'victory',\n",
              "  '.'],\n",
              " [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCXd3FWVmuKe",
        "outputId": "e35bfbaf-340f-4fba-cc39-fa84ce632cda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['soccer',\n",
              "  '-',\n",
              "  'japan',\n",
              "  'get',\n",
              "  'lucky',\n",
              "  'win',\n",
              "  ',',\n",
              "  'china',\n",
              "  'in',\n",
              "  'surprise',\n",
              "  'defeat',\n",
              "  '.'],\n",
              " [0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4R605vAnYT9",
        "outputId": "1b6542ed-4520-4093-dddf-f9404e059917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
        "assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
        "assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
        "\n",
        "assert train_dataset[0][0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Неправильно сформированный train_dataset\"\n",
        "assert train_dataset[0][1] == [3,0,2,0,0,0,2,0,0], \"Неправильно сформированный train_dataset\"\n",
        "\n",
        "assert valid_dataset[0][0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Неправильно сформированный valid_dataset\"\n",
        "assert valid_dataset[0][1] == [0,0,3,0,0,0,0,0,0,0,0], \"Неправильно сформированный valid_dataset\"\n",
        "\n",
        "assert test_dataset[0][0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Неправильно сформированный test_dataset\"\n",
        "assert test_dataset[0][1] == [0,0,1,0,0,0,0,4,0,0,0,0], \"Неправильно сформированный test_dataset\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zP_6iQnQEMC"
      },
      "source": [
        "Реализуем новый `Collator`.\n",
        "\n",
        "Инициализировать коллатор будет 3 аргументами:\n",
        "- токенизатор\n",
        "- параметры токенизатора в виде словаря (затем используем как `**kwargs`)\n",
        "- id спецтокена для последовательностей тегов (значение -1)\n",
        "\n",
        "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
        "\n",
        "На выходе мы хотим получить два тензора:\n",
        "- западденные индексы слов / токенов\n",
        "- западденные индексы тегов\n",
        "\n",
        "**Задание. Реализуйте класс коллатора TransformersCollator.** **<font color='red'>(2 балла)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BonAp65jQEMD"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizer\n",
        "from transformers.tokenization_utils_base import BatchEncoding\n",
        "\n",
        "\n",
        "class TransformersCollator:\n",
        "    \"\"\"\n",
        "    Transformers Collator that handles variable-size sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        tokenizer_kwargs: Dict[str, Any],\n",
        "        label_padding_value: int,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer_kwargs = tokenizer_kwargs\n",
        "\n",
        "        self.label_padding_value = label_padding_value\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        batch: List[Tuple[List[str], List[int]]],\n",
        "    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        tokens, labels = zip(*batch)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        tokens = self.tokenizer(list(tokens), **self.tokenizer_kwargs)\n",
        "        labels = self.encode_labels(tokens, labels, self.label_padding_value)\n",
        "        tokens.pop(\"offset_mapping\")\n",
        "\n",
        "        return tokens, labels\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_labels(\n",
        "        tokens: BatchEncoding,\n",
        "        labels: List[List[int]],\n",
        "        label_padding_value: int,\n",
        "    ) -> torch.LongTensor:\n",
        "\n",
        "        encoded_labels = []\n",
        "\n",
        "        for doc_labels, doc_offset in zip(labels, tokens.offset_mapping):\n",
        "\n",
        "            doc_enc_labels = np.ones(len(doc_offset), dtype=int) * label_padding_value\n",
        "            arr_offset = np.array(doc_offset)\n",
        "\n",
        "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "            encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "        return torch.LongTensor(encoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC8JkUPnQEMD"
      },
      "outputs": [],
      "source": [
        "tokenizer_kwargs = {\n",
        "    \"is_split_into_words\":    True,\n",
        "    \"return_offsets_mapping\": True,\n",
        "    \"padding\":                True,\n",
        "    \"truncation\":             True,\n",
        "    \"max_length\":             512,\n",
        "    \"return_tensors\":         \"pt\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sCDaxR6QEMD"
      },
      "outputs": [],
      "source": [
        "collator = TransformersCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    tokenizer_kwargs=tokenizer_kwargs,\n",
        "    label_padding_value=-1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eirev0N_QEMD"
      },
      "source": [
        "Теперь всё готово, чтобы задать `DataLoader`'ы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JDrLC6pQEME"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collator,\n",
        ")\n",
        "valid_dataloader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
        "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
        "    collate_fn=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3zGjEDHQEME"
      },
      "source": [
        "Посмотрим на то, что мы получили:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSWcYEAWQEME"
      },
      "outputs": [],
      "source": [
        "tokens, labels = next(iter(train_dataloader))\n",
        "\n",
        "tokens = tokens.to(device)\n",
        "labels = labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTcdU1BlQEME",
        "outputId": "94ac83e6-a90e-4247-a8c9-46fb487d6d91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,   184,  1116,  2858,  4043,  1181,  5561,  1179,  5561,   102,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "        [  101,  1103,  3499,  1868,  1149,  1104,  4251,  1105,  1225,  1136,\n",
              "          1138,   170,  2070,  1106,  1840,  1111,  1494,   117,  9466, 15465,\n",
              "           181,  1204,   119,  1122, 20717, 10194,  1810,  1163,   119,   102]],\n",
              "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7ZTh97-QEME",
        "outputId": "7eb951ed-2bf6-475a-f799-07551fc3b5c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1,  3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
              "        [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          3,  0,  0, -1, -1,  4, -1,  8, -1,  0,  0, -1]], device='cuda:0')"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMprtk9bodM9",
        "outputId": "d93f5b97-fbad-4538-f37f-068c7bbac104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "train_tokens, train_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(train_tokens['input_ids'], torch.tensor([[  101,   174,  1358, 22961,   176, 14170,  1840,  1106, 21423,  9304, 10721,  1324,  2495, 12913,   119,   102], [  101, 11109,  1200,  1602,  6715,   102,     0,     0,     0,     0,    0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(train_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(train_labels, torch.tensor([[-1,  3, -1,  0,  2, -1,  0,  0,  0,  2, -1, -1,  0, -1,  0, -1], [-1,  4, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "valid_tokens, valid_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(valid_tokens['input_ids'], torch.tensor([[  101,  5428,   118,  5837, 18117,  5759, 15189,  1321,  1166,  1120,  1499,  1170,  6687,  2681,   119,   102], [  101, 25338, 17996,  1820,   118,  4775,   118,  1476,   102,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(valid_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(valid_labels, torch.tensor([[-1,  0,  0,  3, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1], [-1,  1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "test_tokens, test_labels = next(iter(\n",
        "    torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "    )\n",
        "))\n",
        "assert torch.equal(test_tokens['input_ids'], torch.tensor([[  101,  5862,   118,   179, 26519,  1179,  1243,  6918,  1782,   117,  5144,  1161,  1107,  3774,  3326,   119,   102], [  101,  9468,  3309,  1306, 19122,  2293,   102,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(test_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
        "assert torch.equal(test_labels, torch.tensor([[-1,  0,  0,  1, -1, -1,  0,  0,  0,  0,  4, -1,  0,  0,  0,  0, -1], [-1,  4, -1, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m-taH0SQEMF"
      },
      "source": [
        "В библиотеке **transformers** есть классы для модели BERT, уже настроенные под решение конкретных задач, с соответствующими головами классификации. Для задачи NER будем использовать класс `BertForTokenClassification`.\n",
        "\n",
        "По аналогии с токенизаторами, мы можем использовать класс `AutoModelForTokenClassification`, который по названию модели сам определит, какой класс нужен для инициализации модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6tq_i7JQEMF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vma9yj0zQEMF",
        "outputId": "1057b83c-a634-459f-ba1a-5474578139ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label2idx),\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imv-6gAQQEMG"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAdHfn4oQEMG"
      },
      "outputs": [],
      "source": [
        "outputs = model(**tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-kTke_8QEMG",
        "outputId": "d3ccee80-a0c4-429a-8d62-787faf8543d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert 2 < criterion(outputs[\"logits\"].transpose(1, 2), labels) < 3\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ana4qGKeHrN"
      },
      "outputs": [],
      "source": [
        "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir=f\"logs/Transformer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sNuFPRdQEMH"
      },
      "source": [
        "### Эксперименты\n",
        "\n",
        "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше 0.9.\n",
        "\n",
        "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IfkN20lrN0J"
      },
      "source": [
        "Вы можете использовать ту же самую функцию train, что и до этого за тем исключением, что вместо инференса `model(tokens)` нужно делать `model(**tokens)`, а вместо `outputs` использовать `outputs[\"logits\"].transpose(1, 2)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iyZUFddzYE5"
      },
      "source": [
        "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv92wK5P7pjl"
      },
      "outputs": [],
      "source": [
        "def train_epoch1(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One training cycle (loop).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    for i, (tokens, labels) in tqdm(\n",
        "        enumerate(dataloader),\n",
        "        total=len(dataloader),\n",
        "        desc=\"loop over train batches\",\n",
        "    ):\n",
        "\n",
        "        tokens, labels = tokens.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        tag_scores = model(**tokens)[\"logits\"].transpose(1, 2)\n",
        "        loss = criterion(tag_scores, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "        writer.add_scalar(\n",
        "            \"batch loss / train\", loss.item(), epoch * len(dataloader) + i\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            outputs_inference = model(**tokens)[\"logits\"].transpose(1, 2)\n",
        "            model.train()\n",
        "\n",
        "        batch_metrics = compute_metrics(\n",
        "            outputs=outputs_inference,\n",
        "            labels=labels,\n",
        "        )\n",
        "        for metric_name, metric_value in batch_metrics.items():\n",
        "            batch_metrics_list[metric_name].append(metric_value)\n",
        "            writer.add_scalar(\n",
        "                f\"batch {metric_name} / train\",\n",
        "                metric_value,\n",
        "                epoch * len(dataloader) + i,\n",
        "            )\n",
        "\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    print(f\"Train loss: {avg_loss}\\n\")\n",
        "    writer.add_scalar(\"loss / train\", avg_loss, epoch)\n",
        "\n",
        "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "        metric_value = np.mean(metric_value_list)\n",
        "        if (epoch % 10 == 0):\n",
        "            print(f\"Train {metric_name}: {metric_value}\\n\")\n",
        "        writer.add_scalar(f\"{metric_name} / train\", metric_value, epoch)\n",
        "\n",
        "def evaluate_epoch1(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One evaluation cycle (loop).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (tokens, labels) in tqdm(\n",
        "            enumerate(dataloader),\n",
        "            total=len(dataloader),\n",
        "            desc=\"loop over test batches\",\n",
        "        ):\n",
        "            tokens, labels = tokens.to(device), labels.to(device)\n",
        "            model.zero_grad()\n",
        "            tag_scores = model(**tokens)[\"logits\"].transpose(1, 2)\n",
        "            loss = criterion(tag_scores, labels)\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            writer.add_scalar(\n",
        "                \"batch loss / test\", loss.item(), epoch * len(dataloader) + i\n",
        "            )\n",
        "\n",
        "            batch_metrics = compute_metrics(\n",
        "                outputs=tag_scores,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            for metric_name, metric_value in batch_metrics.items():\n",
        "                batch_metrics_list[metric_name].append(metric_value)\n",
        "                writer.add_scalar(\n",
        "                    f\"batch {metric_name} / test\",\n",
        "                    metric_value,\n",
        "                    epoch * len(dataloader) + i,\n",
        "                )\n",
        "\n",
        "        avg_loss = np.mean(epoch_loss)\n",
        "        print(f\"Test loss:  {avg_loss}\\n\")\n",
        "        writer.add_scalar(\"loss / test\", avg_loss, epoch)\n",
        "\n",
        "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "            metric_value = np.mean(metric_value_list)\n",
        "            if (epoch % 10 == 0):\n",
        "                print(f\"Test {metric_name}: {metric_value}\\n\")\n",
        "            writer.add_scalar(f\"{metric_name} / test\", np.mean(metric_value), epoch)\n",
        "\n",
        "def train1(\n",
        "    n_epochs: int,\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    test_dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Training loop.\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
        "        train_epoch1(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "        )\n",
        "        evaluate_epoch1(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enCU6XJJJ0dV",
        "outputId": "144003c5-79f4-408b-8c1e-3fa64011c491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 14378), started 0:15:43 ago. (Use '!kill 14378' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-3eb13b9046685257\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-3eb13b9046685257\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHXQt1-0J0dV",
        "outputId": "396e2e25-b3fa-44e0-ebac-87b32ef6d377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1 / 50]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches: 100%|██████████████| 7493/7493 [02:32<00:00, 48.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.1334115876942875\n",
            "\n",
            "Train accuracy: 0.9669535474781097\n",
            "\n",
            "Train precision_micro: 0.9669535474781097\n",
            "\n",
            "Train precision_macro: 0.8603354969624792\n",
            "\n",
            "Train precision_weighted: 0.9642163278944871\n",
            "\n",
            "Train recall_micro: 0.9669535474781097\n",
            "\n",
            "Train recall_macro: 0.8621371317464022\n",
            "\n",
            "Train recall_weighted: 0.9669535474781097\n",
            "\n",
            "Train f1_micro: 0.9669535474781097\n",
            "\n",
            "Train f1_macro: 0.8563076205270582\n",
            "\n",
            "Train f1_weighted: 0.9633316717317704\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over test batches: 100%|██████████████| 3683/3683 [00:18<00:00, 196.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss:  0.1515145656496514\n",
            "\n",
            "Test accuracy: 0.9592576310056843\n",
            "\n",
            "Test precision_micro: 0.9592576310056843\n",
            "\n",
            "Test precision_macro: 0.9009999531393967\n",
            "\n",
            "Test precision_weighted: 0.9624064460793542\n",
            "\n",
            "Test recall_micro: 0.9592576310056843\n",
            "\n",
            "Test recall_macro: 0.9000422962504213\n",
            "\n",
            "Test recall_weighted: 0.9592576310056843\n",
            "\n",
            "Test f1_micro: 0.9592576310056843\n",
            "\n",
            "Test f1_macro: 0.8983721695896237\n",
            "\n",
            "Test f1_weighted: 0.9594443339349746\n",
            "\n",
            "Epoch [2 / 50]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loop over train batches:  33%|████▋         | 2485/7493 [00:51<01:43, 48.31it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m \n",
            "Cell \u001b[0;32mIn [177], line 134\u001b[0m, in \u001b[0;36mtrain1\u001b[0;34m(n_epochs, model, train_dataloader, test_dataloader, optimizer, criterion, writer, device)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs): \n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m--> 134\u001b[0m     \u001b[43mtrain_epoch1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     evaluate_epoch1(\n\u001b[1;32m    144\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    145\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m         epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    150\u001b[0m     )\n",
            "Cell \u001b[0;32mIn [177], line 26\u001b[0m, in \u001b[0;36mtrain_epoch1\u001b[0;34m(model, dataloader, optimizer, criterion, writer, device, epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m tag_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(tag_scores, labels)\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:980\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    978\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 980\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    992\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:569\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:347\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    345\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 347\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:285\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    294\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:225\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 225\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m context \u001b[38;5;241m=\u001b[39m unshape(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train1(\n",
        "    n_epochs = 50,\n",
        "    model = model,\n",
        "    train_dataloader = train_dataloader,\n",
        "    test_dataloader = test_dataloader ,\n",
        "    optimizer = optimizer,\n",
        "    criterion = criterion,\n",
        "    writer = writer,\n",
        "    device = device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEYBIsFGJ0dV"
      },
      "outputs": [],
      "source": [
        "evaluate_epoch1(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            criterion=criterion,\n",
        "            writer=writer,\n",
        "            device=device,\n",
        "            epoch=0,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XlI3cb1QEL2"
      },
      "source": [
        "## Часть 4 - Бонус. BiLSTMAttention-теггер (2 баллa)\n",
        "\n",
        "Необходимо провести те же самые эксперименты как и в части 2, но уже с использованием усовершенствованной архитектуры теггера BiLSTM с Attention механизмом.\n",
        "\n",
        "**Обратите внимание**, что реализовывать Attention самому не нужно, можно использовать `torch.nn.MultiheadAttention`.\n",
        "\n",
        "Также сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров и проведите небольшой сравнительный анализ с предыдущей архитектурой. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook).\n",
        "\n",
        "**Задание. Реализуйте класс модели BiLSTMAttn.** **<font color='red'>(1 балл)</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MyLQp047yID"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezh9kLTkQEL9"
      },
      "source": [
        "**Задание. Проведите эксперименты и побейте метрику из части 2.** **<font color='red'>(1 балл)</font>**\n",
        "\n",
        "P.S. Eсли качества увеличить не получилось, это нужно обосновать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE1C1tzEQEL-"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4MIrbmNoQEMA"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "8d4ce941904148077feb793883e611d25d231ca995d9164b22ee99fd0facd8d1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}